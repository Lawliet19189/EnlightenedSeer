{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-7Vbb3-XcPW"
   },
   "source": [
    "# Search suggest autocomplete\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Search_suggest_drop-down_list):\n",
    "> A search suggest drop-down list is a query feature used in computing to show the searcher shortcuts, while the query is typed into a text box. Before the query is complete, a drop-down list with the suggested completions appears to provide options to select. The suggested queries then enable the searcher to complete the required search quickly. As a form of autocompletion, the suggestion list is distinct from search history in that it attempts to be predictive even when the user is searching for the first time. Data may come from popular searches, sponsors, geographic location or other sources.\n",
    "\n",
    "The feature exists in virtually all search interfaces of products we use every day: Google, Slack,  YouTube, etc. Here an example of \"[Google Suggest](https://support.google.com/websearch/answer/106230)\" in action:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1mrwSPEiputqBtwDxJIhPj4-LfbYoxOgA)\n",
    "\n",
    "In this notebook we build a working prototype for such a feature.\n",
    "\n",
    "## Requirements\n",
    "**Input**: Partial query entered by the user, e.g. \"machine lea\" or \"machien learning\". Note that the input may have misspelled or partial words. \n",
    "\n",
    "**Output**: A ranked list of suggested searches, e.g. ```[\"machine learning interview questions\", \"machine learning algorithms\", \"machine learning engineer\", ...]```. If there are a large number of suggestions generated, you may return the top 10 most relevant ones.\n",
    "\n",
    "# Data\n",
    "1. For the first part of this work, we'll use the [MSMARCO Full Web Documents](https://microsoft.github.io/msmarco/) dataset downloadable at [link](https://msmarco.blob.core.windows.net/msmarcoranking/fulldocs.tsv.gz) -- a dataset of web documents indexed by Bing. Note that this dataset does not include any user queries.\n",
    "2. For the second part, we'll use the MSMARCO Queries dataset downloadable at [link](https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1ugcxeeXXM2"
   },
   "source": [
    "## Part 1: Search suggestions using documents\n",
    "For the first user on the system, we'll face a cold-start problem where we don't have access to any past queries. Build a search suggest prototype that leverages the MSMARCO Full Web Documents dataset -- and is able to generate relevant search suggestions without using any user queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntEYMTxxIHdD"
   },
   "source": [
    "### **Proposed Solution:**\n",
    "\n",
    "## **For Demo, please visit [here](http://cde01575bbdf.ngrok.io/)**\n",
    "\n",
    "I believe there are multiple ways to approach this problem. Our Initial Approach is described below:\n",
    "\n",
    "\n",
    "*   **Big Picture**: Generate Artificial queries using document passages, build meaningful graph using the generated queries, and to suggest autocomplete, traverse the query graph and identify relevant queries. \n",
    "*   **Key Advantages**: \n",
    "    * Sub 100ms latency \n",
    "    * Highly Scalable when integrated with graph databases\n",
    "    * Continous learning is comparitively easier than when using DL models\n",
    "    * Since the queries are generated from passages, we have a direct mapping between an autocompleted query and a document. This can be used effectively for search & relevance. \n",
    "    * inherently takes care of multi-langauge.\n",
    "    * Graph structure & properties makes the approach highly flexible and customizable\n",
    "    * can easily integrate with the next iteration when we have user queries.\n",
    "    * can be complemented with retrievers and generators to improve search & relevance.\n",
    "    * Most of the computation happens offline.\n",
    "    * Direct Approach to Auto-correct\n",
    "*   **Key Disadvantages**\n",
    "    * lacks semantic matching during retrieval \n",
    "    * No Obvious way to rank suggested autocomplete queries unless until we have a complementing data like user feedback or user-queries.\n",
    "      * Complementing with OOTB retrievers would fail due to un-clear syntatic representation weightage and not-enough information on the query\n",
    "        * A similar experiment was done and found to highlight this problem during this part of solution\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6AxLkobPr0l"
   },
   "source": [
    "#### Step 1: Generate Artifical Queries\n",
    "\n",
    "*  We will be using [docTTTTTquery model](https://github.com/castorini/docTTTTTquery), trained on MS MARCO Passage Ranking Dataset.\n",
    "*  Before we proceed with the actual Generation, we split the articles into passages. I have not included the script since we would be directly following and using the script provided by [Colbert](https://github.com/stanford-futuredata/ColBERT/blob/master/utility/preprocess/docs2passages.py), this intern is the same implementation adopted from DPR. \n",
    "\n",
    "**Note: This is a very compute-intensive job! due to compute and time constrain, we generate queries only for 40225 passages in MS-marco corpus. This took aproximately 13 hours!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqUaCQM_eQtn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('castorini/doc2query-t5-large-msmarco')\n",
    "model = T5ForConditionalGeneration.from_pretrained('castorini/doc2query-t5-large-msmarco')\n",
    "model = model.to(device)\n",
    "\n",
    "def get_qg(doc_text):\n",
    "\n",
    "    input_ids = tokenizer.encode(doc_text, truncation=True, max_length=512, padding=\"longest\", return_tensors='pt').to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=64,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0,\n",
    "        temperature=0.9,\n",
    "        num_return_sequences=50)\n",
    "\n",
    "    return list(set([tokenizer.decode(outputs[i], skip_special_tokens=True) for i in range(50)]))\n",
    "\n",
    "assert len(get_qg(\"Hello world is a rudementary check for every programming language!\"))!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lcsVbx3RZgw"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Iterative through our passages and generate qeuries.\n",
    "# Store queries in a txt file for further steps\n",
    "with open(\"../data/fulldocs.tsv\", \"rt\") as f_reader:\n",
    "    tsv_reader = csv.reader(f_reader, delimiter=\"\\t\")\n",
    "    with open('../data/qg.txt', 'w') as f:\n",
    "        for row in tqdm(tsv_reader):\n",
    "            qg_list = get_qg(row[-1])\n",
    "            \n",
    "            for question in qg_list:\n",
    "                f.write(question)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKQP1VwHSQ1t"
   },
   "source": [
    "#### STEP 2: Query Graph Building\n",
    "\n",
    "\n",
    "\n",
    "*   The graph connection is as follows:\n",
    "  * Example sentence: ***How do I file taxes*** \n",
    "  * **How** -> **How do** -> **How do I** -> **How do I file** -> **How do I file taxes**\n",
    "\n",
    "*  **Note**: A better graph would be to either represent them as individual tokens or even characters but the traversal, auto-correction, semantic and syntactic correction checks becomes exponentially harder. Therefore, for this prototype, I have went with a reasonable approach which sacrifies space complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBGnmY7JSsva"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein as ln\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX51a9DVSsyz"
   },
   "outputs": [],
   "source": [
    "# pre-compute question & ids map\n",
    "\n",
    "ids_question_map = {}\n",
    "question_ids_map = {}\n",
    "nline = 0\n",
    "\n",
    "with open(\"../data/qg.txt\", \"r\") as f:\n",
    "    with tqdm(total=1563052) as progress_bar:\n",
    "        while line := f.readline():\n",
    "            ids_question_map[nline] = line.strip('\\n').lower()\n",
    "            question_ids_map[line.strip('\\n').lower()] = nline\n",
    "            nline += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "pickle.dump(ids_question_map, open(\"../index/ids_question_map.pkl\", \"wb\"))\n",
    "pickle.dump(question_ids_map, open(\"../index/question_ids_map.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK83M_oKS6WD"
   },
   "outputs": [],
   "source": [
    "ids_question_map = pickle.load(open(\"../index/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/question_ids_map.pkl\", \"rb\"))\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "track_leafs = {}\n",
    "\n",
    "with open(\"../data/qg.txt\", \"r\") as f:\n",
    "    nlines = 0 \n",
    "    with tqdm(total=1563052) as progress_bar:\n",
    "        while line := f.readline():\n",
    "            if line!='':\n",
    "                question = line.strip(\"\\n\").lower()\n",
    "                tokenized = question.split()\n",
    "\n",
    "                if not tokenized:\n",
    "                    continue\n",
    "\n",
    "                prev_node_entry = tokenized[0]\n",
    "                for i in range(2, len(tokenized)+1):\n",
    "\n",
    "                    # Create Nodes and add property Starts, if it's the start of\n",
    "                    # query\n",
    "                    if len(prev_node_entry.split()) == 1:\n",
    "                        if prev_node_entry not in G:\n",
    "                            G.add_node(prev_node_entry, starts=True)\n",
    "                        else:\n",
    "                            G.nodes[prev_node_entry]['starts'] = True\n",
    "                    else:\n",
    "                        if prev_node_entry not in G:\n",
    "                            G.add_node(prev_node_entry, starts=False)\n",
    "                        else:\n",
    "                            G.nodes[prev_node_entry]['starts'] = False\n",
    "\n",
    "                    if len(\" \".join(tokenized[:i]).split()) == 1:\n",
    "                        if \" \".join(tokenized[:i]) not in G:\n",
    "                            G.add_node(\" \".join(tokenized[:i]), starts=True)\n",
    "                        else:\n",
    "                            G.nodes[\" \".join(tokenized[:i])]['starts'] = True\n",
    "                    else:\n",
    "                        if \" \".join(tokenized[:i]) not in G:\n",
    "                            G.add_node(\" \".join(tokenized[:i]), starts=False)\n",
    "                        else:\n",
    "                            G.nodes[\" \".join(tokenized[:i])]['starts'] = False\n",
    "\n",
    "                    # pre-computing leaf nodes, so that we don't have to \n",
    "                    # find descendants during inference\n",
    "                    track_leafs[prev_node_entry] = track_leafs.get(prev_node_entry, []) + question_ids_map[question]\n",
    "                    \n",
    "                    # Add connection between sentence[:i] -> sentence[:i+1]\n",
    "                    G.add_edge(prev_node_entry, \" \".join(tokenized[:i]))\n",
    "\n",
    "                    prev_node_entry =  \" \".join(tokenized[:i])\n",
    "\n",
    "                track_leafs[question] = track_leafs.get(question, []) + question_ids_map[question]\n",
    "                    \n",
    "            progress_bar.update(1)\n",
    "\n",
    "# keep_track of root_nodes. Alternate, much preferred approach is to \n",
    "# have a single root node in the graph which connects to the below nodes.\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]\n",
    "\n",
    "nx.write_gpickle(G, \"../index/query_graph.gpickle\")\n",
    "pickle.dump(track_leafs, open(\"../index/track_leafs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwgPJ5B9WKya"
   },
   "source": [
    "#### Step-3: Inference Pipeline; Traverse Graph for Query Auto-complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fo6nDkuDWvq0"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "# Sample Query \n",
    "query = \"hwat ia the bst way to file\".lower()\n",
    "\n",
    "\n",
    "tokenized_query = query.split()\n",
    "len_tokenized_query = len(tokenized_query)\n",
    "node_list = []\n",
    "\n",
    "def recurssive_search(comparison_term, G, idx=1, root=False, node_list=[], forked=False):\n",
    "    new_term = []\n",
    "    for node in (roots_list if root else G[\" \".join(comparison_term.split()[:-1])]):\n",
    "        \n",
    "        # don't auto-correct on current-word - like google\n",
    "        if ((ln.distance(comparison_term, node) <= 2) if idx!=len_tokenized_query-1 else tokenized_query[-1] in node):\n",
    "            if idx!=len_tokenized_query-1:\n",
    "                node_list.extend(recurssive_search(node + \" \" + tokenized_query[idx+1], G, idx+1, root=False, node_list=node_list, forked=True))\n",
    "            else:\n",
    "                if node in G and node in track_leafs:\n",
    "                    new_term.append(node)\n",
    "\n",
    "              \n",
    "    if forked:\n",
    "        if idx==len_tokenized_query-1:\n",
    "            return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "        return \"\"\n",
    "    else:\n",
    "        if len(tokenized_query)==1:\n",
    "            if idx==len_tokenized_query-1:\n",
    "                return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "        return node_list\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "suggested_queries = recurssive_search(tokenized_query[0], G, idx=0, root=True)\n",
    "\n",
    "# aprox. 0.07ms even for root query search like 'what' \n",
    "print(\"Total time taken: \", timeit.default_timer()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kibSSanCXrNb"
   },
   "source": [
    "#### Step-4: Semantic Ranking/retrieval *(Failed Experiment)*\n",
    "\n",
    "\n",
    "\n",
    "*   Like discussed before, the obvious way to rank the suggested queries would be to sort by similarity between user inference search query & generated search queries \n",
    "*   We make use of Google universal sentence encoder model for encoding the representation and Faiss indexing store for comparison.\n",
    "*   **Note**: Faiss indexing is again an offline process which doesn't affect our inference time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJYPd1MRY2RV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    if isinstance(text, str):\n",
    "        return embed([text]).numpy()\n",
    "    return embed(text).numpy()\n",
    "\n",
    "assert get_embedding(\"Hello World\").shape == (1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrIYJQevY4zi"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "D = 512\n",
    "\n",
    "# A basic indexing scheme\n",
    "index = faiss.IndexFlatL2(D)\n",
    "\n",
    "ids_question_map = {}\n",
    "nline = 0\n",
    "\n",
    "with open(\"../data/qg.txt\", \"r\") as f:\n",
    "    with tqdm(total=1563052) as progress_bar:\n",
    "        while line := f.readline():\n",
    "            index.add(get_embedding(line.strip('\\n')))\n",
    "            ids_question_map[nline] = line.strip('\\n')\n",
    "            nline += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "faiss.write_index(index, \"../index/index.bin\")\n",
    "pickle.dump(ids_question_map, open(\"../index/ids_question_map.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPGkECuWY4lt"
   },
   "outputs": [],
   "source": [
    "# Example Inference Search \n",
    "\n",
    "topk = 4\n",
    "query = \"how do you find the\"\n",
    "\n",
    "dists, ids = index.search(x=get_embedding(query), k=1000)\n",
    "accepted_queries = [(ids_question_map[q], d) for q, d in zip(ids[0], dists[0]) if ids_question_map[q] in qg_list]\n",
    "sorted_accepted_queries = sorted(accepted_queries, key=lambda kv:kv[1], reverse=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlVuAhUiZO8k"
   },
   "source": [
    "##### The problem with this approach is that, \n",
    "*  the correlation between similarity score of encoded representation and the characteristics of the text like semantic and syntatic property is not clearly defined.\n",
    "  * For example, two sentence can have high similarity because of either same intent, same context or similar grammatical structre.\n",
    "*  Sudden shift in semantic and grammatical properties might off-put users during our suggestions.\n",
    "  * example: query -> Machine learning is- #autocomplete#\n",
    "    * suggested query -> is Artificial Intelligence usually interchangeably used with Machine learning\n",
    "    * Observation: There is nothing obviously wrong with the above suggested query. It is relevant to the original query, in-regards to context. but from UI/UX point-of-view, the grammaticial change in structure might spoil the user experience and the retrieved suggestion may or may-not be relevant unless we get more input from the user.\n",
    "    \n",
    "**Note**: The obvious alternative to this ranking would be to incorpurate document trend similar to queries trend which we will do in the next iterative Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ug6XNuJb8jZ"
   },
   "source": [
    "## Part 2: Search suggestions using documents and queries \n",
    "In this part we'll augment the system developed in Part 1 to use the user queries from MSMARCO Queries dataset. Build a search suggest prototype that leverages both queries and documents from our dataset -- and is able to improve on the search suggestions generated by the system developed in Part 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4JzuRb-YYxW"
   },
   "source": [
    "### Proposed Solution\n",
    "\n",
    "We have to **establish couple of assumption** before we proceed on-\n",
    "\n",
    "\n",
    "*   The MSMARCO Queries are considered queries from a tenant instance which contains MSMARCO documents. User-level segregation is not done.\n",
    "  * If we suggest an autocomplete query to a user that is present in our MSMARCO Queries set (ie. the query was already asked in the tenant instance), we aren't suggesting redundant information to the same user but this is highly likely a **different user in the tenant space who has the same question**.\n",
    "*  The **objective reasoning for relevance** would depend on the below factors:\n",
    "  * The **lay syntactic and semantic similarity** between the partial user query and the suggested queries.\n",
    "    * for example. For the query '**what is the best way to connect**', The two suggested responses might be '**what is the best way to connect to an Office VPN**' and other '**for VPN connection in the office, what is the best way**'. We would prefer the first one over the other due it's syntactic similarity to the query.\n",
    "  * Among the suggested queries which includes both artifical queries and tenant queries. **How similar is each of these to tenant's queries trend**? \n",
    "    * ie. If the tenant space is mostly on Machine learning, when user types '**transformers**', we have to assume they are talking about awesome '**huggingface transformers**' library (or) **the architecture**, and not the '**transformers movie**'. (Assuming we have both of these topics discussed in our tenants documents) \n",
    "* Our Ranking approach needs to be flexible and extensible to include more metrics in the future like geogrophical metadata, timing window meta-data, user-role metadata, click-stream etc.\n",
    "  * Once we have more of these information, the obvious step would be to try Learning-to-rank approach.\n",
    "\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "*  **Big Picture:**\n",
    "    *   Like we mentioned in our initial Approach, it is very feasible and reasonable to **extend the queries graph to new user queries**.\n",
    "      * Eventhough, we might be tempted to, there is **no objective reason to prefer user queries more than artifically generated queries**. It should be noted that user queries would be more in-formal and diverse than our generated queries, which is inherently being prefered by algorithm rather than an explicit bias. \n",
    "    *  We use the same approach for building queries graph but this time, instead of starting from scratch, we use the existing artificially generated queries graph.\n",
    "    * Like mentioned earlier, **the user queries describe a trend and topics of interest which we would like to bias our final ranking with**. To extract the trend, **we encode our user search queries, index them with FAISS and run KNN to get centroid representation, which basically is topic representation**.     \n",
    "    * To get a relevance score, we find **cosine similarity between topic representations and suggested queries**. The suggested queries with higher scores are the ones which are more relevant to tenant's trend and in-turn more relevant to user (Assumption).\n",
    "    * If we recall our factors for relevance, we are actually missing one factor, **suggested queries relevance to the users partial input in the search**. To handle this, instead of finding cosine similarity between topic representations and suggested queries, we find similarity between **topic representation augmented with partial input query representation** and **suggested queries**.  \n",
    "      * Instead of having another direct comparison between partial input query representation and suggested queries, we augment them since **our objective is to strengthen the topic representation**.\n",
    "    * One key factor to note is, we are not considering syntactic similarity between query and the suggested autocomplete. This is not implicitly taken care by cosine similarity comparison of encoded representation.\n",
    "      * Therefore to add this, we bias the cosine score with Levenshtein distance, which strongly considers syntactic similarity. \n",
    "    * Like usual, we sort them based on their scores and show top N.\n",
    "\n",
    "### Couple of things to note:\n",
    "* chosing K for kNN is usually important but in-this case, we can make certain assumptions to make it non-consequential. Particularly since our dataset is massive, we choose a large enough k, such that we can atleast guarantee topic diffusion and minimize the chance for noisy cluster. \n",
    "* Again, since we are on the clock, **we encode only first 50,000 user queries** and use it for finding trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDT4IfJ7ll_T"
   },
   "source": [
    "#### STEP 1: Encode the queries and get topic representation (Offline process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70xMdDjxeRc-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    if isinstance(text, str):\n",
    "        return embed([text]).numpy()\n",
    "    return embed(text).numpy()\n",
    "\n",
    "assert get_embedding(\"Hello World\").shape == (1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB87TQFal_Lw"
   },
   "outputs": [],
   "source": [
    "limit = 50000\n",
    "\n",
    "question_ids_map = pickle.load(open(\"../index/question_ids_map.pkl\", \"rb\"))\n",
    "with open(\"../data/queries.train.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader, total=2371783):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            X = np.vstack((X, get_embedding(line)))\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1\n",
    "\n",
    "X = X[1:, :]\n",
    "\n",
    "np.save(open(\"../index/v2/user_queries.npy\", \"wb\"), X)\n",
    "#X = np.load(open(\"../index/v2/user_queries.npy\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laPYJ1-dmFOa"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "D = 512\n",
    "K = 10\n",
    "n_samples = 50000\n",
    "kmeans = faiss.Kmeans(d=D, k=round(16*(n_samples**(1/2))), niter=20, verbose=True, gpu=True)\n",
    "\n",
    "X = np.load(open(\"../index/v2/user_queries.npy\", \"rb\"))\n",
    "\n",
    "kmeans.train(X.astype(np.float32))\n",
    "\n",
    "print(kmeans.centroids.shape)\n",
    "# (3578, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8xFKfinnMAF"
   },
   "source": [
    "#### STEP 2: Extend our queries Graph (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCg1jsfbnYyz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein as ln\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# let's load v1 data\n",
    "\n",
    "G = nx.read_gpickle(\"../index/query_graph.gpickle\")\n",
    "track_leafs = pickle.load(open(\"../index/track_leafs.pkl\", \"rb\"))\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]\n",
    "\n",
    "ids_question_map = pickle.load(open(\"../index/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/question_ids_map.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0QzDyVKnYo5"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "old_lines = len(ids_question_map)\n",
    "lines = len(ids_question_map)\n",
    "with open(\"../data/queries.train.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for row in tsv_reader:\n",
    "        ids_question_map[lines] = row[-1].lower()\n",
    "        question_ids_map[row[-1].lower()] = lines\n",
    "        lines += 1\n",
    "lines - old_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RYbScuwnfX_"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data/queries.train.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader, total=808731):\n",
    "        nlines+=1\n",
    "        line = row[-1]\n",
    "        if line!='':\n",
    "            question = line.strip(\"\\n\").lower()\n",
    "            tokenized = question.split()\n",
    "\n",
    "            if not tokenized:\n",
    "                continue\n",
    "            \n",
    "            prev_node_entry = tokenized[0]\n",
    "            for i in range(2, len(tokenized)+1):\n",
    "              # Create Nodes and add property Starts, if it's the start of\n",
    "              # query\n",
    "              if len(prev_node_entry.split()) == 1:\n",
    "                  if prev_node_entry not in G:\n",
    "                      G.add_node(prev_node_entry, starts=True)\n",
    "                  else:\n",
    "                      G.nodes[prev_node_entry]['starts'] = True\n",
    "              else:\n",
    "                  if prev_node_entry not in G:\n",
    "                      G.add_node(prev_node_entry, starts=False)\n",
    "                  else:\n",
    "                      G.nodes[prev_node_entry]['starts'] = False\n",
    "\n",
    "              if len(\" \".join(tokenized[:i]).split()) == 1:\n",
    "                  if \" \".join(tokenized[:i]) not in G:\n",
    "                      G.add_node(\" \".join(tokenized[:i]), starts=True)\n",
    "                  else:\n",
    "                      G.nodes[\" \".join(tokenized[:i])]['starts'] = True\n",
    "              else:\n",
    "                  if \" \".join(tokenized[:i]) not in G:\n",
    "                      G.add_node(\" \".join(tokenized[:i]), starts=False)\n",
    "                  else:\n",
    "                      G.nodes[\" \".join(tokenized[:i])]['starts'] = False\n",
    "\n",
    "              # pre-computing leaf nodes, so that we don't have to \n",
    "              # find descendants during inference\n",
    "              track_leafs[prev_node_entry] = track_leafs.get(prev_node_entry, []) + question_ids_map[question]\n",
    "              \n",
    "              # Add connection between sentence[:i] -> sentence[:i+1]\n",
    "              G.add_edge(prev_node_entry, \" \".join(tokenized[:i]))\n",
    "\n",
    "              prev_node_entry =  \" \".join(tokenized[:i])\n",
    "\n",
    "          track_leafs[question] = track_leafs.get(question, []) + question_ids_map[question]\n",
    "              \n",
    "\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCES6lx9nfNp"
   },
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, \"../index/v2/query_graph.gpickle\")\n",
    "pickle.dump(track_leafs, open(\"../index/v2/track_leafs.pkl\", \"wb\"))\n",
    "pickle.dump(ids_question_map, open(\"../index/v2/ids_question_map.pkl\", \"wb\"))\n",
    "pickle.dump(question_ids_map, open(\"../index/v2/question_ids_map.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3la19vv5n2kc"
   },
   "source": [
    "#### STEP 3: Inferenece Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6nBzVAhomd7"
   },
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"../index/v2/query_graph.gpickle\")\n",
    "track_leafs = pickle.load(open(\"../index/v2/track_leafs.pkl\", \"rb\"))\n",
    "ids_question_map = pickle.load(open(\"../index/v2/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/v2/question_ids_map.pkl\", \"rb\"))\n",
    "\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]\n",
    "\n",
    "import timeit\n",
    "\n",
    "\n",
    "query = \"bbilical defn\".lower()\n",
    "\n",
    "tokenized_query = query.split()\n",
    "len_tokenized_query = len(tokenized_query)\n",
    "node_list = []\n",
    "\n",
    "def recurssive_search(comparison_term, G, idx=1, root=False, node_list=[], forked=False):\n",
    "    new_term = []\n",
    "    for node in (roots_list if root else G[\" \".join(comparison_term.split()[:-1])]):\n",
    "        \n",
    "        # don't auto-correct on last-word\n",
    "        if ((ln.distance(comparison_term, node) <= 2) if idx!=len_tokenized_query-1 else tokenized_query[-1] in node):\n",
    "            if idx!=len_tokenized_query-1:\n",
    "                node_list.extend(recurssive_search(node + \" \" + tokenized_query[idx+1], G, idx+1, root=False, node_list=node_list, forked=True))\n",
    "            else:\n",
    "                if node in G and node in track_leafs:\n",
    "                    new_term.append(node)\n",
    "\n",
    "              \n",
    "    if forked:\n",
    "        if idx==len_tokenized_query-1:\n",
    "            return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "            \n",
    "        return \"\"\n",
    "    else:\n",
    "        if len(tokenized_query)==1:\n",
    "            if idx==len_tokenized_query-1:\n",
    "                return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "        return node_list\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "suggested_queries = recurssive_search(tokenized_query[0], G, idx=0, root=True)\n",
    "# again, aprox. 0.07ms even for root query search like 'what' \n",
    "print(\"Total time taken: \", timeit.default_timer()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VACYnu8lomYQ"
   },
   "outputs": [],
   "source": [
    "# Ranking the suggested_queries\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ideally, even this would be pre-computed\n",
    "test_x = get_embedding(suggested_queries)\n",
    "\n",
    "\n",
    "# aprox. takes less than 0.01s\n",
    "query_embed = get_embedding(query)\n",
    "query_len = len(query)\n",
    "suggested_queries = np.argsort([c_score*(1/(1+ln.distance(query, suggested_queries[pos][:query_len]))) for pos, c_score in enumerate(np.max(cosine_similarity(test_x, kmeans.centroids + query_embed), axis=-1))])[::-1]\n",
    "# top-10 relevant suggestions\n",
    "for tag in suggested_queries[:10]:\n",
    "    print(suggested_queries[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9pKMEDqcfzp"
   },
   "source": [
    "## Part 3: Evaluation\n",
    "There are many ways of building a search suggest feature that meets the above requirements. In this part, propose evaluation methods to judge a system's performance. \n",
    "\n",
    "How do the two systems developed in Part 1 and Part 2 perform on the proposed evaluation method? \n",
    "\n",
    "What are ways we can improve the above prototypes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg3B2UIIpogm"
   },
   "source": [
    "### Proposed Solution\n",
    "\n",
    "Couple of Notes:\n",
    "\n",
    "*   Our Model hasn't been fully trained on the data. We have used only 10% of the MS-MACRO documents and only 2% of the user queries (for ranking). Therefore, the below evaluation should be considered only as a baseline estimate that we correct once our models can fully utilize the data.\n",
    "*   Evaluation for this is really tricky. It's very easy to fool ourself that the model is not working well based on the evaluations logic. Since autocompletion requires some prior knowledge, both external knowledge (past user queries) and internal knowledge (hints on the partial user query like entities or relevant Noun-Phrases). \n",
    "  * For example execting our model to autocomplete when the partial query is 'what is the best' is not the right expectation. A better query that we should expect our model to autocomplete would be 'what is the best way to file taxes in-`, this partial query can be autocompleted if our approach can pick what region the tenant/user resides-in. A more similar general queries would be like- 'what is the difference between AI and-' \n",
    "\n",
    "* Due to time constrain and incomplete trained model, we run the evaluation only for 1000 sample queries.\n",
    "\n",
    "### Big Picture:\n",
    "\n",
    "*   We will be using the MS-MARCO queries development set to evaluate our model performance.\n",
    "*   For each queries, we will break them into contiguous partial query and try to auto-complete the rest. \n",
    "*   One thing to note is, we consider to predict and evaluate a partial query only if it contains a Noun Phrase.\n",
    "*   We will use Exact Match and F1 score to compare the autocompleted query and the original query. \n",
    "  * If our query is \"how long is a flight from chicago to australia\" and our partial query is \"how long is a flight from chicago\". assume the predicted Autocompletion is \"how long is a flight from chicago to US\". We compute the two metrics by passing the following information: EM(\"to US\", \"to australia\") and F1(\"to US\", \"to australia\")  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UuEPInGzedC"
   },
   "source": [
    "#### Step 1: Initialize necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJZQKE3szqBN"
   },
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "import pandas as pd\n",
    "import Levenshtein as ln\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXY5kZQgzr2t"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    if isinstance(text, str):\n",
    "        return embed([text]).numpy()\n",
    "    return embed(text).numpy()\n",
    "\n",
    "get_embedding(\"Hello World\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKGv728QeSM4"
   },
   "outputs": [],
   "source": [
    "# prediction Code\n",
    "\n",
    "import Levenshtein as ln\n",
    "\n",
    "\n",
    "def predict_query(query, G, roots_list, ids_question_map, track_leafs):\n",
    "    tokenized_query = query.split()\n",
    "    len_tokenized_query = len(tokenized_query)\n",
    "    node_list = []\n",
    "\n",
    "    def recurssive_search(comparison_term, G, idx=1, root=False, node_list=[], forked=False):\n",
    "        new_term = []\n",
    "        for node in (roots_list if root else G[\" \".join(comparison_term.split()[:-1])]):\n",
    "\n",
    "            # don't auto-correct on last-word\n",
    "            if ((ln.distance(comparison_term, node) <= 2) if idx != len_tokenized_query - 1 else tokenized_query[\n",
    "                                                                                                     -1] in node):\n",
    "                if idx != len_tokenized_query - 1:\n",
    "                    node_list.extend(recurssive_search(node + \" \" + tokenized_query[idx + 1], G, idx + 1, root=False,\n",
    "                                                       node_list=node_list, forked=True))\n",
    "                else:\n",
    "                    if node in G and node in track_leafs:\n",
    "                        new_term.append(node)\n",
    "\n",
    "        if forked:\n",
    "            if idx == len_tokenized_query - 1:\n",
    "                return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "\n",
    "            return \"\"\n",
    "        else:\n",
    "            if len(tokenized_query) == 1:\n",
    "                if idx == len_tokenized_query - 1:\n",
    "                    return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "            return node_list\n",
    "\n",
    "    res = recurssive_search(tokenized_query[0], G, idx=0, root=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0AG_GjTznmw"
   },
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8HFaaaSzwcm"
   },
   "source": [
    "#### Step 2: Evaluate v1, where the train data is only MS-Marco docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNRMi697znfi"
   },
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"../index/query_graph.gpickle\")\n",
    "track_leafs = pickle.load(open(\"../index/track_leafs.pkl\", \"rb\"))\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]\n",
    "\n",
    "ids_question_map = pickle.load(open(\"../index/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/question_ids_map.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVkqbxCeznVy"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "limit = 1000\n",
    "metrics = {\n",
    "    \"EM\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    }\n",
    "}\n",
    "with open(\"../data/queries.dev.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            question = line\n",
    "            tokenized_question = line.split()\n",
    "            doc = nlp(question)\n",
    "            np_list = [str(token) for token in doc if token.tag_[:2]==\"NN\"]\n",
    "            observed_np = False\n",
    "            for token_idx in range(1, len(tokenized_question)):\n",
    "                if observed_np or tokenized_question[token_idx-1] in np_list:\n",
    "                    observed_np = True\n",
    "                    suggested_queries = predict_query(\" \".join(tokenized_question[:token_idx]), G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "\n",
    "                    if suggested_queries!=[]:\n",
    "                        for k in [3, 5, 10]:\n",
    "                            temp_EM, temp_F1 = [0], [0]\n",
    "                            for q in suggested_queries[:k]:\n",
    "                                temp_EM.append(compute_exact_match(q, question))\n",
    "                                temp_F1.append(compute_f1(q, question))\n",
    "                            metrics['EM']['@'+str(k)] = metrics['EM']['@'+str(k)] + [max(temp_EM)]\n",
    "                            metrics['F1']['@'+str(k)] = metrics['F1']['@'+str(k)] + [max(temp_F1)]\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARF8XuE-0MAz"
   },
   "outputs": [],
   "source": [
    "#metrics\n",
    "print(\"Average Exact Match Metric: \", \"\\n\", \"EM@3: \"+str(np.average(metrics['EM']['@3'])), \"\\n\", \"EM@5: \"+str(np.average(metrics['EM']['@5'])), \"\\n\", \"EM@10: \"+str(np.average(metrics['EM']['@10'])))\n",
    "print(\"--------\")\n",
    "print(\"Average F1 Metric: \", \"\\n\", \"F1@3: \"+str(np.average(metrics['F1']['@3'])), \"\\n\", \"F1@5: \"+str(np.average(metrics['F1']['@5'])), \"\\n\", \"F1@10: \"+str(np.average(metrics['F1']['@10'])))\n",
    "\n",
    "## Output\n",
    "\n",
    "\"\"\"\n",
    "Average Exact Match Metric:  \n",
    " EM@3: 0.009933774834437087 \n",
    " EM@5: 0.011037527593818985 \n",
    " EM@10: 0.012141280353200883\n",
    "--------\n",
    "Average F1 Metric:  \n",
    " F1@3: 0.4550674975638583 \n",
    " F1@5: 0.47319383050217734 \n",
    " F1@10: 0.4945249375231127\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOV7liqx0UJo"
   },
   "source": [
    "#### Step 3: Evaluate v2, where the train data is MS-Marco docs + MS-Marco queries.\n",
    "* Note: We are not ranking the final suggestions yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8Q9Zvp40gcO"
   },
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"../index/v2/query_graph.gpickle\")\n",
    "track_leafs = pickle.load(open(\"../index/v2/track_leafs.pkl\", \"rb\"))\n",
    "ids_question_map = pickle.load(open(\"../index/v2/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/v2/question_ids_map.pkl\", \"rb\"))\n",
    "\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEfV3z0c0j6j"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#limit = 15140\n",
    "limit = 1000\n",
    "metrics = {\n",
    "    \"EM\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    }\n",
    "}\n",
    "with open(\"../data/queries.dev.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            question = line\n",
    "            tokenized_question = line.split()\n",
    "            doc = nlp(question)\n",
    "            np_list = [str(token) for token in doc if token.tag_[:2]==\"NN\"]\n",
    "            observed_np = False\n",
    "            for token_idx in range(1, len(tokenized_question)):\n",
    "                if observed_np or tokenized_question[token_idx-1] in np_list:\n",
    "                    observed_np = True\n",
    "                    suggested_queries = predict_query(\" \".join(tokenized_question[:token_idx]), G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "\n",
    "                    if suggested_queries!=[]:\n",
    "                        for k in [3, 5, 10]:\n",
    "                            temp_EM, temp_F1 = [0], [0]\n",
    "                            for q in suggested_queries[:k]:\n",
    "                                temp_EM.append(compute_exact_match(q, question))\n",
    "                                temp_F1.append(compute_f1(q, question))\n",
    "                            metrics['EM']['@'+str(k)] = metrics['EM']['@'+str(k)] + [max(temp_EM)]\n",
    "                            metrics['F1']['@'+str(k)] = metrics['F1']['@'+str(k)] + [max(temp_F1)]\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9Z1tJ1a0jzN"
   },
   "outputs": [],
   "source": [
    "#metrics\n",
    "print(\"Average Exact Match Metric: \", \"\\n\", \"EM@3: \"+str(np.average(metrics['EM']['@3'])), \"\\n\", \"EM@5: \"+str(np.average(metrics['EM']['@5'])), \"\\n\", \"EM@10: \"+str(np.average(metrics['EM']['@10'])))\n",
    "print(\"--------\")\n",
    "print(\"Average F1 Metric: \", \"\\n\", \"F1@3: \"+str(np.average(metrics['F1']['@3'])), \"\\n\", \"F1@5: \"+str(np.average(metrics['F1']['@5'])), \"\\n\", \"F1@10: \"+str(np.average(metrics['F1']['@10'])))\n",
    "\n",
    "## Output\n",
    "\n",
    "\"\"\"\n",
    "Average Exact Match Metric:  \n",
    " EM@3: 0.009149130832570906 \n",
    " EM@5: 0.010064043915827997 \n",
    " EM@10: 0.011893870082342177\n",
    "--------\n",
    "Average F1 Metric:  \n",
    " F1@3: 0.4879684855782274 \n",
    " F1@5: 0.5054822473192558 \n",
    " F1@10: 0.5287107502350595\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqwu2Al_0ulg"
   },
   "source": [
    "#### Step 3: Evaluate v2 + ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoPdsAiJ0jce"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "X = np.load(open(\"../index/v2/user_queries.npy\", \"rb\"))\n",
    "\n",
    "D = 512\n",
    "K = 10\n",
    "kmeans = faiss.Kmeans(d=D, k=round(16*(X.shape[0]**(1/2))), niter=20, verbose=True, gpu=True)\n",
    "\n",
    "kmeans.train(X.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dxm1LYWW01WX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#limit = 15140\n",
    "limit = 1000\n",
    "metrics = {\n",
    "    \"EM\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    }\n",
    "}\n",
    "with open(\"../data/queries.dev.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            question = line\n",
    "            tokenized_question = line.split()\n",
    "            if tokenized_question!=[]:\n",
    "                query_embed = get_embedding(question)\n",
    "                \n",
    "                doc = nlp(question)\n",
    "                np_list = [str(token) for token in doc if token.tag_[:2]==\"NN\"]\n",
    "                observed_np = False\n",
    "                for token_idx in range(1, len(tokenized_question)):\n",
    "                    if observed_np or tokenized_question[token_idx-1] in np_list:\n",
    "                        observed_np = True\n",
    "                        suggested_queries = predict_query(\" \".join(tokenized_question[:token_idx]), G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "\n",
    "                        if suggested_queries!=[]:\n",
    "                            test_x = get_embedding(suggested_queries)\n",
    "                            query_len = len(\" \".join(tokenized_question[:token_idx]))\n",
    "                            sorted_res = np.argsort([c_score*(1/(1+ln.distance(\" \".join(tokenized_question[:token_idx]), suggested_queries[pos][:query_len]))) for pos, c_score in enumerate(np.max(cosine_similarity(test_x, kmeans.centroids + query_embed), axis=-1))])[::-1]\n",
    "                            for k in [3, 5, 10]:\n",
    "                                temp_EM, temp_F1 = [0], [0]\n",
    "                                filtered_queries = [suggested_queries[tag] for tag in sorted_res[:k]]\n",
    "                                for q in filtered_queries[:k]:\n",
    "                                    temp_EM.append(compute_exact_match(q, question))\n",
    "                                    temp_F1.append(compute_f1(q, question))\n",
    "                                metrics['EM']['@'+str(k)] = metrics['EM']['@'+str(k)] + [max(temp_EM)]\n",
    "                                metrics['F1']['@'+str(k)] = metrics['F1']['@'+str(k)] + [max(temp_F1)]\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0A9fPWjN01RU"
   },
   "outputs": [],
   "source": [
    "#metrics\n",
    "print(\"Average Exact Match Metric: \", \"\\n\", \"EM@3: \"+str(np.average(metrics['EM']['@3'])), \"\\n\", \"EM@5: \"+str(np.average(metrics['EM']['@5'])), \"\\n\", \"EM@10: \"+str(np.average(metrics['EM']['@10'])))\n",
    "print(\"--------\")\n",
    "print(\"Average F1 Metric: \", \"\\n\", \"F1@3: \"+str(np.average(metrics['F1']['@3'])), \"\\n\", \"F1@5: \"+str(np.average(metrics['F1']['@5'])), \"\\n\", \"F1@10: \"+str(np.average(metrics['F1']['@10'])))\n",
    "\n",
    "# Output\n",
    "\n",
    "\"\"\"\n",
    "Average Exact Match Metric:  \n",
    " EM@3: 0.013723696248856358 \n",
    " EM@5: 0.013723696248856358 \n",
    " EM@10: 0.013723696248856358\n",
    "--------\n",
    "Average F1 Metric:  \n",
    " F1@3: 0.5759839235621964 \n",
    " F1@5: 0.58476066024534 \n",
    " F1@10: 0.5911137329097126\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YNlFrxs0-ZV"
   },
   "source": [
    "### Observations\n",
    "\n",
    "\n",
    "*   We can see that our model has improved over each iteration. Training the model on full dataset would definitely improve it even more and give a good estimate of how well the approach performs.\n",
    "\n",
    "### Improvements to the protype\n",
    "\n",
    "* Eventhough I believe our graph could theoretically scale very well. The space complexity is comparitively high. Finding to make use of character-wise or word-wise query graph would be useful. \n",
    "* Our current graph traversal doesn't take into consideration, the semantic similarities of tokens. It would be good experiment to think this through and evaluate whether such a improvement is necessary. \n",
    "* We have used a lot of extra files to trace some information. This decision was taken due to limitation of in-memory graph. Moving to a distributed graph storage like Dgraph would help us store lot more information in the graph.\n",
    "* Eventhough we haven't explored Auto-regressive DL models in this prototype, it would definitely be in the next step to improve the approach. We have both user queries and artificially generated queries. Training a Gpt-2 like model to auto-complete query would be a good addition. \n",
    "\n",
    "## For Demo, please visit [here](http://cde01575bbdf.ngrok.io/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzONJcoi3u0z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ntEYMTxxIHdD",
    "2Ug6XNuJb8jZ",
    "Yg3B2UIIpogm",
    "3UuEPInGzedC",
    "b8HFaaaSzwcm",
    "FOV7liqx0UJo",
    "Uqwu2Al_0ulg"
   ],
   "name": "Sri - Search suggest autocomplete",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
