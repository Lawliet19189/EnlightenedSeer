{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction Code\n",
    "\n",
    "import Levenshtein as ln\n",
    "\n",
    "\n",
    "def predict_query(query, G, roots_list, ids_question_map, track_leafs):\n",
    "    tokenized_query = query.split()\n",
    "    len_tokenized_query = len(tokenized_query)\n",
    "    node_list = []\n",
    "\n",
    "    def recurssive_search(comparison_term, G, idx=1, root=False, node_list=[], forked=False):\n",
    "        new_term = []\n",
    "        for node in (roots_list if root else G[\" \".join(comparison_term.split()[:-1])]):\n",
    "\n",
    "            # don't auto-correct on last-word\n",
    "            if ((ln.distance(comparison_term, node) <= 2) if idx != len_tokenized_query - 1 else tokenized_query[\n",
    "                                                                                                     -1] in node):\n",
    "                if idx != len_tokenized_query - 1:\n",
    "                    node_list.extend(recurssive_search(node + \" \" + tokenized_query[idx + 1], G, idx + 1, root=False,\n",
    "                                                       node_list=node_list, forked=True))\n",
    "                else:\n",
    "                    if node in G and node in track_leafs:\n",
    "                        new_term.append(node)\n",
    "\n",
    "        if forked:\n",
    "            if idx == len_tokenized_query - 1:\n",
    "                return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "\n",
    "            return \"\"\n",
    "        else:\n",
    "            if len(tokenized_query) == 1:\n",
    "                if idx == len_tokenized_query - 1:\n",
    "                    return [ids_question_map[child] for term in new_term for child in track_leafs[term]]\n",
    "            return node_list\n",
    "\n",
    "    res = recurssive_search(tokenized_query[0], G, idx=0, root=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-cookie",
   "metadata": {},
   "source": [
    "# Version 1 Evaluation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prospective-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "import pandas as pd\n",
    "import Levenshtein as ln\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "downtown-onion",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-18 18:41:10.412006: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-18 18:41:10.412185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:10.412852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.62GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2021-07-18 18:41:10.412914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:10.413535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.62GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2021-07-18 18:41:10.413558: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-18 18:41:10.415025: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-18 18:41:10.415079: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-18 18:41:10.416009: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-18 18:41:10.416207: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-18 18:41:10.419135: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-07-18 18:41:10.420055: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-18 18:41:10.420329: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-18 18:41:10.420522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:10.421265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:10.421946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:10.422611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:10.423242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-07-18 18:41:11.680280: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-18 18:41:11.682814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:11.683191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.62GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2021-07-18 18:41:11.683279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:11.683965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.62GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2021-07-18 18:41:11.684031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:11.684417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:11.685097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:11.685851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:11.686459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-07-18 18:41:11.686496: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-18 18:41:12.320108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-18 18:41:12.320131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2021-07-18 18:41:12.320137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y \n",
      "2021-07-18 18:41:12.320141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N \n",
      "2021-07-18 18:41:12.320307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:12.320694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:12.321030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:12.321366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:12.321696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:12.321991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2021-07-18 18:41:12.322281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-18 18:41:12.322694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8164 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2021-07-18 18:41:17.613152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-18 18:41:17.845575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3199980000 Hz\n",
      "2021-07-18 18:41:21.547781: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-18 18:41:21.718615: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    if isinstance(text, str):\n",
    "        return embed([text]).numpy()\n",
    "    return embed(text).numpy()\n",
    "\n",
    "get_embedding(\"Hello World\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "catholic-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"../index/query_graph.gpickle\")\n",
    "track_leafs = pickle.load(open(\"../index/track_leafs.pkl\", \"rb\"))\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]\n",
    "\n",
    "ids_question_map = pickle.load(open(\"../index/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/question_ids_map.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outstanding-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eleven-america",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:34, 28.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "limit = 1000\n",
    "metrics = {\n",
    "    \"EM\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    }\n",
    "}\n",
    "with open(\"../data/queries.dev.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            question = line\n",
    "            tokenized_question = line.split()\n",
    "            doc = nlp(question)\n",
    "            np_list = [str(token) for token in doc if token.tag_[:2]==\"NN\"]\n",
    "            observed_np = False\n",
    "            for token_idx in range(1, len(tokenized_question)):\n",
    "                if observed_np or tokenized_question[token_idx-1] in np_list:\n",
    "                    observed_np = True\n",
    "                    suggested_queries = predict_query(\" \".join(tokenized_question[:token_idx]), G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "\n",
    "                    if suggested_queries!=[]:\n",
    "                        for k in [3, 5, 10]:\n",
    "                            temp_EM, temp_F1 = [0], [0]\n",
    "                            for q in suggested_queries[:k]:\n",
    "                                temp_EM.append(compute_exact_match(q, question))\n",
    "                                temp_F1.append(compute_f1(q, question))\n",
    "                            metrics['EM']['@'+str(k)] = metrics['EM']['@'+str(k)] + [max(temp_EM)]\n",
    "                            metrics['F1']['@'+str(k)] = metrics['F1']['@'+str(k)] + [max(temp_F1)]\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fabulous-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"what is\"\n",
    "# suggested_queries = predict_query(\"what is\", G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "# sorted(suggested_queries + [query])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electrical-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match Metric:  \n",
      " EM@3: 0.009933774834437087 \n",
      " EM@5: 0.011037527593818985 \n",
      " EM@10: 0.012141280353200883\n",
      "--------\n",
      "Average F1 Metric:  \n",
      " F1@3: 0.4550674975638583 \n",
      " F1@5: 0.47319383050217734 \n",
      " F1@10: 0.4945249375231127\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "print(\"Average Exact Match Metric: \", \"\\n\", \"EM@3: \"+str(np.average(metrics['EM']['@3'])), \"\\n\", \"EM@5: \"+str(np.average(metrics['EM']['@5'])), \"\\n\", \"EM@10: \"+str(np.average(metrics['EM']['@10'])))\n",
    "print(\"--------\")\n",
    "print(\"Average F1 Metric: \", \"\\n\", \"F1@3: \"+str(np.average(metrics['F1']['@3'])), \"\\n\", \"F1@5: \"+str(np.average(metrics['F1']['@5'])), \"\\n\", \"F1@10: \"+str(np.average(metrics['F1']['@10'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upset-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match Metric:  \n",
      " EM@3: 0.0040885860306643955 \n",
      " EM@5: 0.004429301533219762 \n",
      " EM@10: 0.005792163543441226\n",
      "--------\n",
      "Average F1 Metric:  \n",
      " F1@3: 0.3896572010693722 \n",
      " F1@5: 0.4050902330327353 \n",
      " F1@10: 0.4225743728587715\n"
     ]
    }
   ],
   "source": [
    "## Without checking for NP\n",
    "# Average Exact Match Metric:  \n",
    "#  EM@3: 0.0040885860306643955 \n",
    "#  EM@5: 0.004429301533219762 \n",
    "#  EM@10: 0.005792163543441226\n",
    "# --------\n",
    "# Average F1 Metric:  \n",
    "#  F1@3: 0.3896572010693722 \n",
    "#  F1@5: 0.4050902330327353 \n",
    "#  F1@10: 0.4225743728587715"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-horizon",
   "metadata": {},
   "source": [
    "# v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein as ln\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "egyptian-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"../index/v2/query_graph.gpickle\")\n",
    "track_leafs = pickle.load(open(\"../index/v2/track_leafs.pkl\", \"rb\"))\n",
    "ids_question_map = pickle.load(open(\"../index/v2/ids_question_map.pkl\", \"rb\"))\n",
    "question_ids_map = pickle.load(open(\"../index/v2/question_ids_map.pkl\", \"rb\"))\n",
    "\n",
    "roots_list = [i for i,j in G.nodes(data=\"starts\", default=1) if j==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "greenhouse-music",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [01:05, 15.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#limit = 15140\n",
    "limit = 1000\n",
    "metrics = {\n",
    "    \"EM\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    }\n",
    "}\n",
    "with open(\"../data/queries.dev.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            question = line\n",
    "            tokenized_question = line.split()\n",
    "            doc = nlp(question)\n",
    "            np_list = [str(token) for token in doc if token.tag_[:2]==\"NN\"]\n",
    "            observed_np = False\n",
    "            for token_idx in range(1, len(tokenized_question)):\n",
    "                if observed_np or tokenized_question[token_idx-1] in np_list:\n",
    "                    observed_np = True\n",
    "                    suggested_queries = predict_query(\" \".join(tokenized_question[:token_idx]), G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "\n",
    "                    if suggested_queries!=[]:\n",
    "                        for k in [3, 5, 10]:\n",
    "                            temp_EM, temp_F1 = [0], [0]\n",
    "                            for q in suggested_queries[:k]:\n",
    "                                temp_EM.append(compute_exact_match(q, question))\n",
    "                                temp_F1.append(compute_f1(q, question))\n",
    "                            metrics['EM']['@'+str(k)] = metrics['EM']['@'+str(k)] + [max(temp_EM)]\n",
    "                            metrics['F1']['@'+str(k)] = metrics['F1']['@'+str(k)] + [max(temp_F1)]\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smart-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match Metric:  \n",
      " EM@3: 0.009149130832570906 \n",
      " EM@5: 0.010064043915827997 \n",
      " EM@10: 0.011893870082342177\n",
      "--------\n",
      "Average F1 Metric:  \n",
      " F1@3: 0.4879684855782274 \n",
      " F1@5: 0.5054822473192558 \n",
      " F1@10: 0.5287107502350595\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "print(\"Average Exact Match Metric: \", \"\\n\", \"EM@3: \"+str(np.average(metrics['EM']['@3'])), \"\\n\", \"EM@5: \"+str(np.average(metrics['EM']['@5'])), \"\\n\", \"EM@10: \"+str(np.average(metrics['EM']['@10'])))\n",
    "print(\"--------\")\n",
    "print(\"Average F1 Metric: \", \"\\n\", \"F1@3: \"+str(np.average(metrics['F1']['@3'])), \"\\n\", \"F1@5: \"+str(np.average(metrics['F1']['@5'])), \"\\n\", \"F1@10: \"+str(np.average(metrics['F1']['@10'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "directed-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match Metric:  \n",
      " EM@3: 0.004072681704260651 \n",
      " EM@5: 0.0043859649122807015 \n",
      " EM@10: 0.005639097744360902\n",
      "--------\n",
      "Average F1 Metric:  \n",
      " F1@3: 0.40197607565187854 \n",
      " F1@5: 0.4187706908725181 \n",
      " F1@10: 0.4385090217213102\n"
     ]
    }
   ],
   "source": [
    "## Without checking for NP\n",
    "# Average Exact Match Metric:  \n",
    "#  EM@3: 0.004072681704260651 \n",
    "#  EM@5: 0.0043859649122807015 \n",
    "#  EM@10: 0.005639097744360902\n",
    "# --------\n",
    "# Average F1 Metric:  \n",
    "#  F1@3: 0.40197607565187854 \n",
    "#  F1@5: 0.4187706908725181 \n",
    "#  F1@10: 0.4385090217213102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-crowd",
   "metadata": {},
   "source": [
    "# v2 + Ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hungry-sigma",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 50001 points to 3578 centroids: please provide at least 139542 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 50001 points in 512D to 3578 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.01 s\n",
      "  Iteration 19 (0.78 s, search 0.56 s): objective=24707.2 imbalance=1.461 nsplit=0       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24707.16015625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "X = np.load(open(\"../index/v2/user_queries.npy\", \"rb\"))\n",
    "\n",
    "D = 512\n",
    "K = 10\n",
    "kmeans = faiss.Kmeans(d=D, k=round(16*(X.shape[0]**(1/2))), niter=20, verbose=True, gpu=True)\n",
    "\n",
    "kmeans.train(X.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "female-richards",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:52,  5.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#limit = 15140\n",
    "limit = 1000\n",
    "metrics = {\n",
    "    \"EM\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    },\n",
    "    \"F1\": {\n",
    "        \"@3\": [],\n",
    "        \"@5\": [],\n",
    "        \"@10\": []\n",
    "    }\n",
    "}\n",
    "with open(\"../data/queries.dev.tsv\", \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    nlines = 0 \n",
    "    for row in tqdm(tsv_reader):\n",
    "        nlines += 1\n",
    "        line = row[-1].lower()\n",
    "        if line!='':\n",
    "            question = line\n",
    "            tokenized_question = line.split()\n",
    "            if tokenized_question!=[]:\n",
    "                query_embed = get_embedding(question)\n",
    "                doc = nlp(question)\n",
    "                np_list = [str(token) for token in doc if token.tag_[:2]==\"NN\"]\n",
    "                observed_np = False\n",
    "                for token_idx in range(1, len(tokenized_question)):\n",
    "                    if observed_np or tokenized_question[token_idx-1] in np_list:\n",
    "                        observed_np = True\n",
    "                        suggested_queries = predict_query(\" \".join(tokenized_question[:token_idx]), G, roots_list, ids_question_map, track_leafs)[:500]\n",
    "\n",
    "                        if suggested_queries!=[]:\n",
    "                            test_x = get_embedding(suggested_queries)\n",
    "                            sorted_res = np.argsort(np.max(cosine_similarity(test_x, kmeans.centroids + query_embed), axis=-1))[::-1]\n",
    "                            for k in [3, 5, 10]:\n",
    "                                temp_EM, temp_F1 = [0], [0]\n",
    "                                filtered_queries = [suggested_queries[tag] for tag in sorted_res[:k]]\n",
    "                                for q in filtered_queries[:k]:\n",
    "                                    temp_EM.append(compute_exact_match(q, question))\n",
    "                                    temp_F1.append(compute_f1(q, question))\n",
    "                                metrics['EM']['@'+str(k)] = metrics['EM']['@'+str(k)] + [max(temp_EM)]\n",
    "                                metrics['F1']['@'+str(k)] = metrics['F1']['@'+str(k)] + [max(temp_F1)]\n",
    "        if not limit:\n",
    "            break\n",
    "        else:\n",
    "            limit-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "balanced-smooth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexReplicas; proxy of <Swig Object of type 'faiss::IndexReplicasTemplate< faiss::Index > *' at 0x7fbda6f050f0> >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tropical-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match Metric:  \n",
      " EM@3: 0.013723696248856358 \n",
      " EM@5: 0.013723696248856358 \n",
      " EM@10: 0.013723696248856358\n",
      "--------\n",
      "Average F1 Metric:  \n",
      " F1@3: 0.5759839235621964 \n",
      " F1@5: 0.58476066024534 \n",
      " F1@10: 0.5911137329097126\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "print(\"Average Exact Match Metric: \", \"\\n\", \"EM@3: \"+str(np.average(metrics['EM']['@3'])), \"\\n\", \"EM@5: \"+str(np.average(metrics['EM']['@5'])), \"\\n\", \"EM@10: \"+str(np.average(metrics['EM']['@10'])))\n",
    "print(\"--------\")\n",
    "print(\"Average F1 Metric: \", \"\\n\", \"F1@3: \"+str(np.average(metrics['F1']['@3'])), \"\\n\", \"F1@5: \"+str(np.average(metrics['F1']['@5'])), \"\\n\", \"F1@10: \"+str(np.average(metrics['F1']['@10'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "selective-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match Metric:  \n",
      " EM@3: 0.007832080200501253 \n",
      " EM@5: 0.007832080200501253 \n",
      " EM@10: 0.007832080200501253\n",
      "--------\n",
      "Average F1 Metric:  \n",
      " F1@3: 0.4952691105325539 \n",
      " F1@5: 0.5116865949954855 \n",
      " F1@10: 0.5286050616505952\n"
     ]
    }
   ],
   "source": [
    "## Without checking for NP\n",
    "# Average Exact Match Metric:  \n",
    "#  EM@3: 0.007832080200501253 \n",
    "#  EM@5: 0.007832080200501253 \n",
    "#  EM@10: 0.007832080200501253\n",
    "# --------\n",
    "# Average F1 Metric:  \n",
    "#  F1@3: 0.4952691105325539 \n",
    "#  F1@5: 0.5116865949954855 \n",
    "#  F1@10: 0.5286050616505952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seer",
   "language": "python",
   "name": "seer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
